# /configs/config.yaml

# Modular training: swap task, model, and dataset
defaults:
  - _self_
  - task: tri_cdr               # override on CLI: task=bin_cdr / gender / handedness / age
  - model: 3D_CNN               # override on CLI: model=<another>
  - datamodule: dti             # override on CLI: data=<another>

seed: 42
test: false

logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: logs
  name: ${hydra:runtime.choices.task}-${hydra:runtime.choices.model} # TODO: fix naming

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: ${task.monitor}     # e.g., val_auroc or val_mae
  mode: ${task.mode}           # "max" for AUROC, "min" for MAE
  patience: 5
  min_delta: 0.001

checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: ${task.monitor}
  mode: ${task.mode}
  save_top_k: 1
  filename: best-{epoch:02d}

# DataModule selection comes entirely from the data group
datamodule: ${datamodule.module}

# Model selection comes entirely from the model group
model: ${model.module}

trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 100
  accelerator: auto
  devices: 1
  log_every_n_steps: 10
