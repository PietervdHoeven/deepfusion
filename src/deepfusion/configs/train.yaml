# /configs/config.yaml

# Modular training: swap task, model, and dataset
defaults:
  - _self_
  - task: recon               # override on CLI: task=bin_cdr / gender / handedness / age / tri_cdr
  - model: AutoEncoder3D               # override on CLI: model=<another>
  - datamodule: ae             # override on CLI: data=<another>

seed: 42
test: false

ckpt_path: null            # path to checkpoint to resume training from (optional)

logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: logs
  name: ${hydra:runtime.choices.task}-${hydra:runtime.choices.model} # TODO: fix naming

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: ${task.monitor}     # e.g., val_auroc or val_mae
  mode: ${task.mode}           # "max" for AUROC, "min" for MAE
  patience: 8                  # 8 epochs of no improvement allows for LRonPlateau to reduce LR e-3 -> e-4 -> e-5 -> e-6 with patience 3 each
  min_delta: 0.001

checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  monitor: ${task.monitor}
  mode: ${task.mode}
  save_top_k: 1
  filename: best-epoch:{epoch:02d}-val_loss:{val_loss:.4f}
  auto_insert_metric_name: false

# DataModule selection comes entirely from the data group
datamodule: ${datamodule.module}

# Model selection comes entirely from the model group
model: ${model.module}

trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 100
  accelerator: auto
  devices: 1
  log_every_n_steps: 10
  precision: 16-mixed                  # mixed precision training
